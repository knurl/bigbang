registryCredentials:
  enabled: true
  registry: {{HelmRegistry}}
  username: {{HelmRepoUser}}
  password: {{HelmRepoPassword}}

resources:
  requests:
    memory: {{hive_mem}}
    cpu: {{hive_cpu}}
  limits:
    memory: {{hive_mem}}
    cpu: {{hive_cpu}}

objectStorage:
  {% if Target == "aws" %}
  awsS3:
    region: {{Region}}
    endpoint: s3.{{Region}}.amazonaws.com
    # For AWS, we don't need to pass credentials because we've assigned a role
    # with a policy that allows full access to S3
  {% elif Target == "az" %}
  azure:
    abfs:
      authType: "accessKey"
      accessKey:
        storageAccount: {{StorageAccount}}
        accessKey: {{adls_access_key}}
  {% elif Target == "gcp" %}
  gs:
    cloudKeyFileSecret: {{gcskey}}
  {% endif %}

# External database, so tolerant to spot instances
database:
  type: external
  external:
    jdbcUrl: jdbc:postgresql://{{hmsdb_address}}:{{postgres_port}}/{{DBNameHms}}
    driver: org.postgresql.Driver
    user: {{hmsdb_user}}
    password: {{DBPassword}}

{% if Target == 'az' %}
# Make sure to prioritise failure-tolerant pods to the spot instances
tolerations:
- key: "kubernetes.azure.com/scalesetpriority"
  operator: "Equal"
  value: "spot"
  effect: "NoSchedule"
{% endif %}

# The K8S scheduler sometimes makes sub-optimal arrangements in which hive and
# the cache service are scheduled to the same nodes, not leaving enough room
# for either the coordinator or a worker. Prevent this with an anti-affinity
# rule.
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app
              operator: In
              values:
                - starburst-cache-service
        topologyKey: "kubernetes.io/hostname"
